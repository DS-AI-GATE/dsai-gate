# üìò Machine Learning ‚Äì Topic-Wise Guide

### üìö Resources

Reference : https://github.com/khangich/machine-learning-interview/tree/master

* **Multicollinearity:**
  [Statistics by Jim](https://statisticsbyjim.com/regression/multicollinearity-in-regression-analysis/) | [YouTube](https://www.youtube.com/watch?v=Cba9LJ9lS8s)

* **Feature Scaling:**
  [Sebastian Raschka Blog](https://sebastianraschka.com/Articles/2014_about_feature_scaling.html)

* **Random Forest vs GBDT:**
  [Medium Comparison Article](https://medium.com/@aravanshad/gradient-boosting-versus-random-forest-cfa3fa8f0d80)

* **SMOTE (Synthetic Minority Over-sampling Technique):**
  [Original Paper ‚Äì Arxiv](https://arxiv.org/pdf/1106.1813.pdf)

* **Discriminative vs Generative Models:**
  [Medium Article](https://medium.com/@mlengineer/generative-and-discriminative-models-af5637a66a3)
  [Stanford Paper by Ng & Jordan](http://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf)

* **Logistic Regression:**
  [YouTube ‚Äì Coding Explained](https://www.youtube.com/watch?v=-la3q9d7AKQ)
  *üìù Try implementing from scratch in NumPy ‚Äì bonus for vectorized version and a MapReduce variant!*
  Sample Code: [`logistic_regression.ipynb`](https://github.com/khangich/machine-learning-interview/blob/master/sample/logistic_regression.ipynb)

* **Quantile Regression:**
  [YouTube Lecture](https://www.youtube.com/watch?v=s203ScTy4xQ&t=954s)

* **L1/L2 Regularization Intuition:**
  [LinkedIn Visual Intuition](https://www.linkedin.com/pulse/intuitive-visual-explanation-differences-between-l1-l2-xiaoli-chen/)

* **Decision Trees & Random Forests:**
  [MIT Slides](https://people.csail.mit.edu/dsontag/courses/ml16/slides/lecture11.pdf)

* **Boosting Fundamentals:**
  [Stanford Slides by Hastie](https://web.stanford.edu/~hastie/TALKS/boost.pdf)

* **Least Squares as MLE:**
  [YouTube Video](https://www.youtube.com/watch?v=_-Gnu498s3o)

* **MLE Introduction:**
  [YouTube Lecture](https://www.youtube.com/watch?v=WflqTUOvdik&t=15s)

* **K-Means Clustering:**
  [Stanford Notes](https://stanford.edu/~cpiech/cs221/handouts/kmeans.html)
  Sample Code: [`kmeans.ipynb`](https://github.com/khangich/machine-learning-interview/blob/master/sample/kmeans.ipynb)
  *üìù Implement from scratch ‚Äì optimize initialization (e.g., k-means++).*

* **PCA Intuition:**
  [Neuronal Blog ‚Äì PCA Explained](http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/)

* **Eigenvectors & Eigenvalues Visualization:**
  [Setosa.io](https://setosa.io/ev/eigenvectors-and-eigenvalues/)

* **SVD (Singular Value Decomposition):**
  [Gregory Gundersen Blog](https://gregorygundersen.com/blog/2018/12/10/svd/)

* **KL-Divergence:**
  [CountBayesie Blog](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained)

* **Cross Entropy:**
  [Colah‚Äôs Blog](https://colah.github.io/posts/2015-09-Visual-Information/)

* **Cheatsheet:**
  [ML Flashcards](https://machinelearningflashcards.com/) ‚Äì optional but useful for quick revision.

---

## üéì Interview Experiences

### üßë‚Äçüè´ IIT MSR/AI Program Insights

* [IIT Bombay & Delhi (MS ‚Äì MINDS & CMInDS) Experience](https://gateoverflow.in/blog/14429/iit-bombay-%26-delhi-ms-interview-experience-minds-%26-cminds)
* [IIT Indore MSR Experience (Reddit)](https://www.reddit.com/r/GATEtard/comments/1kfakyn/iit_indore_msr_interview_experience/)
* [IIT Delhi MSR AI Interview Blog](https://gateoverflow.in/blog/15907/iit-delhi-masters-research-ai-experience)

---
