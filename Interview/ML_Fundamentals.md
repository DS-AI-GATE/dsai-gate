
### ML fundamentals
* [Collinearity](https://statisticsbyjim.com/regression/multicollinearity-in-regression-analysis/) and [read more](https://www.youtube.com/watch?v=Cba9LJ9lS8s)
* [Features scaling](https://sebastianraschka.com/Articles/2014_about_feature_scaling.html)
* [Random forest vs GBDT](https://medium.com/@aravanshad/gradient-boosting-versus-random-forest-cfa3fa8f0d80)
* [SMOTE synthetic minority over-sampling technique](https://arxiv.org/pdf/1106.1813.pdf)
* [Compare discriminative vs generative model](https://medium.com/@mlengineer/generative-and-discriminative-models-af5637a66a3) and [extra read](http://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf)
* [Logistic regression](https://www.youtube.com/watch?v=-la3q9d7AKQ). Try to implement logistic regression from scratch. Bonus point for vectorized version in numpy + completed in 20 minutes [sample code from martinpella](sample/logistic_regression.ipynb). Followup with MapReduce version. 
* [Quantile regression](https://www.youtube.com/watch?v=s203ScTy4xQ&t=954s)
* [L1/L2 intuition](https://www.linkedin.com/pulse/intuitive-visual-explanation-differences-between-l1-l2-xiaoli-chen/)
* [Decision tree and Random Forest fundamental](https://people.csail.mit.edu/dsontag/courses/ml16/slides/lecture11.pdf)
* [Explain boosting](https://web.stanford.edu/~hastie/TALKS/boost.pdf)
* [Least Square as Maximum Likelihood Estimator](https://www.youtube.com/watch?v=_-Gnu498s3o)
* [Maximum Likelihood Estimator introduction](https://www.youtube.com/watch?v=WflqTUOvdik&t=15s)
* [Kmeans](https://stanford.edu/~cpiech/cs221/handouts/kmeans.html). Try to implement Kmeans from scratch [sample code from flothesof.github.io](sample/kmeans.ipynb). Bonus point for vectorized version in numpy + completed in 20 minutes. Follow-up with worst case time complexity and improvement for [initialization](extra.md).
* Fundamentals about [PCA](http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/)
* I didn't use [flashcard](https://machinelearningflashcards.com/) but I'm sure it helps up to certain extend.
* [Eigen Values and Eigen Vectors Visualized](https://setosa.io/ev/eigenvectors-and-eigenvalues/)
* [Singular Value Decomposition](https://gregorygundersen.com/blog/2018/12/10/svd/)
* [KL-Divergence Explained :  KLD is an idea that you will run into often in your ML journey](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained)
* [what is cross entropy? ](https://colah.github.io/posts/2015-09-Visual-Information/)



### Interview Experiences and Questions 

https://gateoverflow.in/blog/14429/iit-bombay-%26-delhi-ms-interview-experience-minds-%26-cminds
https://www.reddit.com/r/GATEtard/comments/1kfakyn/iit_indore_msr_interview_experience/
https://gateoverflow.in/blog/15907/iit-delhi-masters-research-ai-experience
